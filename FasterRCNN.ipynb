{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557d0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DataSet\n",
    "import utils.augmentation as aug\n",
    "import utils.helpers as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2539078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision \n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np \n",
    "import os, random, time, json, math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mean_average_precision import MetricBuilder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from ImageEnhancement import MSRCR, FUSION, CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6049284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 800 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 20 # number of epochs to train for\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# training images and XML files directory\n",
    "SEED = 42\n",
    "TEST_RATIO = 0.1 # for train/test split\n",
    "\n",
    "MAPPING = {\n",
    "    'Kartoffel': 0,\n",
    "    'Fish': 1,\n",
    "    'Cnidaria':2   \n",
    "}\n",
    "\n",
    "# whether to visualize images after crearing the data loaders\n",
    "VISUALIZE_TRANSFORMED_IMAGES = False\n",
    "# location to save model and plots\n",
    "OUT_DIR = '/home/anton/Documents/Thesis/temResults/'\n",
    "SAVE_PLOTS_EPOCH = 5 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 5 # save model after these many epochs\n",
    "\n",
    "PREPARE_TEST_DATA = True\n",
    "\n",
    "IMAGE_DIRECTORY = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c22635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all relevant paths for images and their respective label files and splitting them \n",
    "#      into train and validation datasets\n",
    "\n",
    "base_dir = \"/home/anton/Downloads/image_annotator/linux_v1.4.3/data/oldclean/\"\n",
    "\n",
    "imgs, labels = helpers.image_and_label_paths(base_dir, image_dir = \"images\", label_dir = \"labels\")\n",
    "inputs_train, inputs_valid, targets_train, targets_valid = train_test_split(imgs, labels, test_size=TEST_RATIO, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9ed1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataSet:\n",
    "# Augmentations from the Albumentations library\n",
    "albumentations_augmentations = A.Compose([ A.Flip(0.45), A.RandomRotate90(0.5),\n",
    "                                           A.MotionBlur(p=0.2), A.Blur(blur_limit=3, p=0.1)],\n",
    "                                         bbox_params={  'format': 'pascal_voc', \n",
    "                                                        'label_fields': ['labels']}\n",
    "                                        )\n",
    "\n",
    "# custum augmentations combined with those from the library\n",
    "my_albumenations = aug.Augmenter([aug.Clip, aug.Resize(800, 800), aug.Normalize], \n",
    "                                                 albumentations_augmentations)\n",
    "\n",
    "# create DataSet with correct sample lists\n",
    "train_dataset = DataSet(inputs_train, \n",
    "                            targets_train, \n",
    "                            use_cache          = False,\n",
    "                            transform          = my_albumenations,\n",
    "                            mapping            = MAPPING,\n",
    "                            random_enhancement = False\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "# Validation DataSet:\n",
    "# Augmentations from the Albumentations library\n",
    "albumentations_augmentations = A.Compose([],bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "# custum augmentations combined with those from the library\n",
    "my_albumenations = aug.Augmenter([aug.Clip, aug.Resize(800, 800), aug.Normalize], \n",
    "                                                 albumentations_augmentations)\n",
    "\n",
    "\n",
    "# create DataSet object with correct sample lists\n",
    "validation_dataset = DataSet(inputs_valid, \n",
    "                                targets_valid, \n",
    "                                use_cache          = False,\n",
    "                                transform          = my_albumenations,\n",
    "                                mapping            = MAPPING, \n",
    "                                random_enhancement = False)          \n",
    "\n",
    "# Example for accessing entries in a DataSet object:\n",
    "#train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e7790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# A Torch dataloader takes our dataset object to successively extract same sized batches from it \n",
    "train_loader = DataLoader(  dataset     = train_dataset,\n",
    "                            batch_size  = BATCH_SIZE,\n",
    "                            shuffle     = True,\n",
    "                            num_workers = 0,\n",
    "                            collate_fn  = collate_fn\n",
    "                         )\n",
    "\n",
    "\n",
    "valid_loader = DataLoader(  dataset     = validation_dataset,\n",
    "                            batch_size  = BATCH_SIZE,\n",
    "                            shuffle     = False,\n",
    "                            num_workers = 0,\n",
    "                            collate_fn  = collate_fn\n",
    "                         )\n",
    "\n",
    "#train_loss_hist = Averager()\n",
    "#val_loss_hist = Averager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681b35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates model objects and download pretrained weights\n",
    "def create_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c28c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that executes training for one epoch\n",
    "def train(train_data_loader, model, optimizer, train_itr, train_loss_list):\n",
    "    print('Training')\n",
    "    loss_avg = 0\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        loss_avg += loss_value\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list, train_itr, (1.0*loss_avg)/len(train_data_loader)\n",
    "\n",
    "# function that executes validation for one epoch\n",
    "def validate(valid_data_loader, model, val_itr, val_loss_list):\n",
    "    print('Validating')\n",
    "    loss_avg = 0\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        loss_avg += loss_value\n",
    "        val_itr += 1\n",
    "\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list, val_itr, (1.0*loss_avg)/len(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00dd12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 20\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904db3c195d6406a88a1605bda80aad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/Projects/DS_studies/Thesis/envs/ml/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23024/1354859702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# start timer and carry out training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch #{epoch} train loss: {train_loss_avg:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23024/2043327000.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data_loader, model, optimizer, train_itr, train_loss_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_itr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/DS_studies/Thesis/envs/ml/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/DS_studies/Thesis/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'model'\n",
    "MODEL_APPENDIX = \"RANDOM_FINAL\"\n",
    "\n",
    "model = create_model(num_classes = len(MAPPING.keys()) )\n",
    "\n",
    "### FOR IMPORTING A MODEL\n",
    "#model.load_state_dict(torch.load(OUT_DIR+\"/FILE-NAME.pth\"))\n",
    "###\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# define the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.003, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss_list, train_itr, train_loss_avg = train(train_loader, model, optimizer, train_itr, train_loss_list)\n",
    "    val_loss_list, val_itr, val_loss_avg = validate(valid_loader, model, val_itr, val_loss_list)\n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_avg:.3f}\")   \n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_avg:.3f}\")   \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    if (epoch+1) % SAVE_MODEL_EPOCH == 0 or (epoch+1) == NUM_EPOCHS: # save model after every n epochs or at end\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model_{MODEL_APPENDIX}_{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n",
    "    if (epoch+1) % SAVE_PLOTS_EPOCH == 0 or (epoch+1) == NUM_EPOCHS: # save loss plots after n epochs or at end\n",
    "        figure_1, train_ax = plt.subplots()\n",
    "        figure_2, valid_ax = plt.subplots()\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{MODEL_APPENDIX}_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{MODEL_APPENDIX}_{epoch+1}.png\")\n",
    "        print('SAVING PLOTS COMPLETE...') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc0531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c663a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b865c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
